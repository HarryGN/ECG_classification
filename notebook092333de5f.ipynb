{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom copy import deepcopy\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom torch.optim.lr_scheduler import StepLR","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:20:49.778038Z","iopub.execute_input":"2023-01-09T09:20:49.778352Z","iopub.status.idle":"2023-01-09T09:20:52.532797Z","shell.execute_reply.started":"2023-01-09T09:20:49.778278Z","shell.execute_reply":"2023-01-09T09:20:52.531767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data loading and processing","metadata":{}},{"cell_type":"code","source":"#load csv files to pandas dataframe\ntrain_df = pd.read_csv('/kaggle/input/heartbeat/mitbih_train.csv', header=None)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:20:58.875194Z","iopub.execute_input":"2023-01-09T09:20:58.875725Z","iopub.status.idle":"2023-01-09T09:21:12.03009Z","shell.execute_reply.started":"2023-01-09T09:20:58.875689Z","shell.execute_reply":"2023-01-09T09:21:12.029021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the last column is the data label described as category from 0 to 4\n#so we can use it as a target\ny_df = train_df.iloc[:, -1]\ny_df = y_df.astype('int')\ny_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:21:17.37616Z","iopub.execute_input":"2023-01-09T09:21:17.37651Z","iopub.status.idle":"2023-01-09T09:21:17.391665Z","shell.execute_reply.started":"2023-01-09T09:21:17.37648Z","shell.execute_reply":"2023-01-09T09:21:17.390634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the rest are the ecg datapoints so we use them as features\nX_df = train_df.iloc[:, :-1]\nX_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:21:21.3355Z","iopub.execute_input":"2023-01-09T09:21:21.336402Z","iopub.status.idle":"2023-01-09T09:21:21.368715Z","shell.execute_reply.started":"2023-01-09T09:21:21.336367Z","shell.execute_reply":"2023-01-09T09:21:21.367624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#convert all feautures to float type\nX_df = X_df.astype('float')\nX_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:21:26.785013Z","iopub.execute_input":"2023-01-09T09:21:26.785383Z","iopub.status.idle":"2023-01-09T09:21:26.857628Z","shell.execute_reply.started":"2023-01-09T09:21:26.785351Z","shell.execute_reply":"2023-01-09T09:21:26.856538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we actually dont need to one hot encode the target since we are using cross entropy loss and it runs more efficiently when we pass in the target as indices\n#print out the unique values in the target\nunique, counts = np.unique(y_df, return_counts=True)\nprint(f'unique values: {unique}')\nprint(f'counts: {counts}')","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:21:31.225296Z","iopub.execute_input":"2023-01-09T09:21:31.225685Z","iopub.status.idle":"2023-01-09T09:21:31.234102Z","shell.execute_reply.started":"2023-01-09T09:21:31.225653Z","shell.execute_reply":"2023-01-09T09:21:31.232923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#make a dictionary to map the target indices to class names\nclass_names = {0: 'N', 1: 'S', 2: 'V', 3: 'F', 4: 'Q'}","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:21:34.608891Z","iopub.execute_input":"2023-01-09T09:21:34.609269Z","iopub.status.idle":"2023-01-09T09:21:34.614623Z","shell.execute_reply.started":"2023-01-09T09:21:34.609236Z","shell.execute_reply":"2023-01-09T09:21:34.613435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot the first 5 ecg signals in the dataset and their corresponding labels \nfig, ax = plt.subplots(5, 1, figsize=(20, 10))\nfor i in range(5):\n    ax[i].plot(X_df.iloc[i, :])\n    ax[i].set_title(f'Label: {class_names[y_df[i]]}')\n    ax[i].set_xlabel('Time (ms)')\n    ax[i].set_ylabel('Amplitude (mV)')","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:21:48.144533Z","iopub.execute_input":"2023-01-09T09:21:48.145229Z","iopub.status.idle":"2023-01-09T09:21:48.79517Z","shell.execute_reply.started":"2023-01-09T09:21:48.145193Z","shell.execute_reply":"2023-01-09T09:21:48.794207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the input dims are 187 and the output dims are 5\n#we want the input to be (batch_size, 1, 187) and the output to be (batch_size, 5) because the input has one channel and the output has 5 classes\nX_np = X_df.to_numpy()\nX_np.shape","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:21:56.251622Z","iopub.execute_input":"2023-01-09T09:21:56.251977Z","iopub.status.idle":"2023-01-09T09:21:56.258392Z","shell.execute_reply.started":"2023-01-09T09:21:56.251948Z","shell.execute_reply":"2023-01-09T09:21:56.257453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we want to add a channel dimension to the input\nX_np = X_np.reshape(-1, 1, 187)\nX_np.shape","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:21:57.364439Z","iopub.execute_input":"2023-01-09T09:21:57.365119Z","iopub.status.idle":"2023-01-09T09:21:57.371607Z","shell.execute_reply.started":"2023-01-09T09:21:57.365082Z","shell.execute_reply":"2023-01-09T09:21:57.370685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#do the same for the target\ny_np = y_df.to_numpy()\ny_np.shape","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:21:58.250136Z","iopub.execute_input":"2023-01-09T09:21:58.250838Z","iopub.status.idle":"2023-01-09T09:21:58.257083Z","shell.execute_reply.started":"2023-01-09T09:21:58.2508Z","shell.execute_reply":"2023-01-09T09:21:58.256166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_np = y_np.reshape(-1, 1)\ny_np.shape","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:21:59.031002Z","iopub.execute_input":"2023-01-09T09:21:59.031362Z","iopub.status.idle":"2023-01-09T09:21:59.038191Z","shell.execute_reply.started":"2023-01-09T09:21:59.031332Z","shell.execute_reply":"2023-01-09T09:21:59.037269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data normalization","metadata":{}},{"cell_type":"code","source":"#perfect, we now have the input and target in the correct shape\nX_mean = X_np.mean()\nX_std = X_np.std()\nX_max = X_np.max()\nX_min = X_np.min()\nprint(f'mean before normalization: {X_mean}')\nprint(f'std: {X_std}')\nprint(f'max: {X_max}')","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:22:03.118971Z","iopub.execute_input":"2023-01-09T09:22:03.12015Z","iopub.status.idle":"2023-01-09T09:22:03.231467Z","shell.execute_reply.started":"2023-01-09T09:22:03.120095Z","shell.execute_reply":"2023-01-09T09:22:03.230369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we need to subtract the mean from the data\nX_norm = X_np - X_mean\n#then multiply by 1/std to make the std = 1\nX_norm *= 1/X_std","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:22:05.465687Z","iopub.execute_input":"2023-01-09T09:22:05.46681Z","iopub.status.idle":"2023-01-09T09:22:05.521968Z","shell.execute_reply.started":"2023-01-09T09:22:05.466765Z","shell.execute_reply":"2023-01-09T09:22:05.520721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_norm.shape)\nprint(f'new mean is {X_norm.mean()}')\nprint(f'new std is {X_norm.std()}')","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:22:08.438035Z","iopub.execute_input":"2023-01-09T09:22:08.43861Z","iopub.status.idle":"2023-01-09T09:22:08.528108Z","shell.execute_reply.started":"2023-01-09T09:22:08.438551Z","shell.execute_reply":"2023-01-09T09:22:08.527018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now plot the normalized data\nfig, ax = plt.subplots(5, 1, figsize=(20, 10))\nfor i in range(5):\n    ax[i].plot(X_norm[i, 0, :])\n    ax[i].set_title(f'Label: {class_names[y_df[i]]}')\n    ax[i].set_xlabel('Time (ms)')\n    ax[i].set_ylabel('Amplitude (mV)')","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:22:12.533383Z","iopub.execute_input":"2023-01-09T09:22:12.534063Z","iopub.status.idle":"2023-01-09T09:22:13.292359Z","shell.execute_reply.started":"2023-01-09T09:22:12.534026Z","shell.execute_reply":"2023-01-09T09:22:13.291465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now plot the distribution of data \nfig, ax = plt.subplots(1, 1, figsize=(20, 10))\nax.hist(X_norm.flatten(), bins=100)\nax.set_title(f'Distribution of data')\nax.set_xlabel('Time (ms)')\nax.set_ylabel('Amplitude (mV)')","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:22:19.415048Z","iopub.execute_input":"2023-01-09T09:22:19.415844Z","iopub.status.idle":"2023-01-09T09:22:20.182581Z","shell.execute_reply.started":"2023-01-09T09:22:19.415801Z","shell.execute_reply":"2023-01-09T09:22:20.181544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_norm.shape #representing the input data as a sequence of 187 points","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:22:24.195209Z","iopub.execute_input":"2023-01-09T09:22:24.195868Z","iopub.status.idle":"2023-01-09T09:22:24.203734Z","shell.execute_reply.started":"2023-01-09T09:22:24.195806Z","shell.execute_reply":"2023-01-09T09:22:24.20253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_np.shape #representing the target as indices 0 to 4","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:22:26.577782Z","iopub.execute_input":"2023-01-09T09:22:26.578145Z","iopub.status.idle":"2023-01-09T09:22:26.585487Z","shell.execute_reply.started":"2023-01-09T09:22:26.578114Z","shell.execute_reply":"2023-01-09T09:22:26.584427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving the normalized variables","metadata":{}},{"cell_type":"code","source":"#saving the normalized data to a csv file\nnp.savetxt('X_norm.csv', X_norm.squeeze(), delimiter=',')\nnp.savetxt('y_np.csv', y_np, delimiter=',')\n# np.loadtxt('X_norm.csv', delimiter=',').shape","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:22:29.585589Z","iopub.execute_input":"2023-01-09T09:22:29.587131Z","iopub.status.idle":"2023-01-09T09:22:43.852482Z","shell.execute_reply.started":"2023-01-09T09:22:29.587088Z","shell.execute_reply":"2023-01-09T09:22:43.851426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the normalized variables from disk","metadata":{}},{"cell_type":"code","source":"X_norm = np.loadtxt('X_norm.csv', delimiter=',').reshape(-1, 1, 187)\ny_np = np.loadtxt('y_np.csv', delimiter=',').reshape(-1, 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_norm.shape)\nprint(y_np.shape)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:22:58.128839Z","iopub.execute_input":"2023-01-09T09:22:58.129815Z","iopub.status.idle":"2023-01-09T09:22:58.135689Z","shell.execute_reply.started":"2023-01-09T09:22:58.129776Z","shell.execute_reply":"2023-01-09T09:22:58.134506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#change y_np data type to int\ny_np = y_np.astype('int')\ny_np.dtype","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:23:01.076094Z","iopub.execute_input":"2023-01-09T09:23:01.076452Z","iopub.status.idle":"2023-01-09T09:23:01.086006Z","shell.execute_reply.started":"2023-01-09T09:23:01.076422Z","shell.execute_reply":"2023-01-09T09:23:01.084003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## We split the data into training and validation sets using scikit-learn","metadata":{}},{"cell_type":"code","source":"#shuffle the data and split into train and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_norm, y_np, test_size=0.2, shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:23:10.987666Z","iopub.execute_input":"2023-01-09T09:23:10.988609Z","iopub.status.idle":"2023-01-09T09:23:11.167951Z","shell.execute_reply.started":"2023-01-09T09:23:10.988543Z","shell.execute_reply":"2023-01-09T09:23:11.166938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now we check for class imbalance","metadata":{}},{"cell_type":"code","source":"#print the frequency of each class in the train and validation sets\nprint(f'Frequency of each class in the train set')\nunique, counts = np.unique(y_train, return_counts=True)\nfor i in range(len(unique)):\n    print(f'{class_names[unique[i]]}: {counts[i]}')\n\nprint(f'Frequency of each class in the validation set')\nunique, counts = np.unique(y_val, return_counts=True)\nfor i in range(len(unique)):\n    print(f'{class_names[unique[i]]}: {counts[i]}')","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:23:15.514125Z","iopub.execute_input":"2023-01-09T09:23:15.514823Z","iopub.status.idle":"2023-01-09T09:23:15.524724Z","shell.execute_reply.started":"2023-01-09T09:23:15.514786Z","shell.execute_reply":"2023-01-09T09:23:15.523586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We define a hyper-parameters dictionary","metadata":{}},{"cell_type":"code","source":"hyper_params = {'bs': 1024,\n                                'lr': 1e-3,\n                                'lr_decay': 0.3,\n                                'epochs':50,\n                                }","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:23:27.866306Z","iopub.execute_input":"2023-01-09T09:23:27.86671Z","iopub.status.idle":"2023-01-09T09:23:27.87199Z","shell.execute_reply.started":"2023-01-09T09:23:27.866675Z","shell.execute_reply":"2023-01-09T09:23:27.870848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pytorch dataset and dataloader\ntrain_ds = torch.utils.data.TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\nval_ds = torch.utils.data.TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:23:32.737834Z","iopub.execute_input":"2023-01-09T09:23:32.738219Z","iopub.status.idle":"2023-01-09T09:23:32.792556Z","shell.execute_reply.started":"2023-01-09T09:23:32.738189Z","shell.execute_reply":"2023-01-09T09:23:32.791602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_ds), len(val_ds)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:23:36.053656Z","iopub.execute_input":"2023-01-09T09:23:36.05443Z","iopub.status.idle":"2023-01-09T09:23:36.061944Z","shell.execute_reply.started":"2023-01-09T09:23:36.054387Z","shell.execute_reply":"2023-01-09T09:23:36.060709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We compute the weight of each class by inverting their distribution and normalizing it","metadata":{}},{"cell_type":"code","source":"#the class distribution is not uniform so we have to define weights for each class\nclass_dist = np.unique(y_train, return_counts=True)[1] #index 1 returns the counts\nclass_dist = class_dist/np.sum(class_dist) #normalize to get 0-1 values\n# normalize the weights\nweights = 1/class_dist #invert to compensate for class distribution imbalance\nweights = weights/weights.sum() #normalize to get values 0-1\nweights, class_names","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:23:41.594039Z","iopub.execute_input":"2023-01-09T09:23:41.594406Z","iopub.status.idle":"2023-01-09T09:23:41.605389Z","shell.execute_reply.started":"2023-01-09T09:23:41.594376Z","shell.execute_reply":"2023-01-09T09:23:41.604103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We assign each example in the training set the weight we computed in the above cell","metadata":{}},{"cell_type":"code","source":"#get weights for each example in the dataset\ndata_weights = np.zeros(y_train.shape[0]) #shape (m, )\nfor i in range(y_train.shape[0]): #for each example we assign the corresponding weight\n    data_weights[i] = weights[y_train[i, 0]]\ndata_weights[:10]","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:23:46.449961Z","iopub.execute_input":"2023-01-09T09:23:46.450327Z","iopub.status.idle":"2023-01-09T09:23:46.488439Z","shell.execute_reply.started":"2023-01-09T09:23:46.450297Z","shell.execute_reply":"2023-01-09T09:23:46.487335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Then we pass the weights to the WeightedRandomSampler to sample according to the normalized weights","metadata":{}},{"cell_type":"code","source":"#data loader - we use the random weights sampler to balance the classes\ntrain_dl = DataLoader(train_ds, batch_size=hyper_params['bs'], sampler=WeightedRandomSampler(weights=data_weights, num_samples= len(y_train), replacement=True))\nval_dl = DataLoader(val_ds, batch_size=hyper_params['bs'])\n","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:23:54.169067Z","iopub.execute_input":"2023-01-09T09:23:54.169902Z","iopub.status.idle":"2023-01-09T09:23:54.176345Z","shell.execute_reply.started":"2023-01-09T09:23:54.169864Z","shell.execute_reply":"2023-01-09T09:23:54.175127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We check the training dataloader for class distributions","metadata":{}},{"cell_type":"code","source":"#get the class distribution in dataloader\nfor batch in train_dl:\n    print(batch[1].shape)\n    unique, counts = np.unique(batch[1], return_counts=True)\n    for i in range(len(unique)):\n        print(f'{class_names[unique[i]]}: {counts[i]}')\n    break","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:23:56.865944Z","iopub.execute_input":"2023-01-09T09:23:56.867669Z","iopub.status.idle":"2023-01-09T09:23:56.920886Z","shell.execute_reply.started":"2023-01-09T09:23:56.867616Z","shell.execute_reply":"2023-01-09T09:23:56.919643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the distribution is nearly uniform now","metadata":{}},{"cell_type":"code","source":"#sample random batch from the train dataloader and plot the first 5 ecg signals in the batch\nx, y = next(iter(train_dl))\nfig, ax = plt.subplots(5, 1, figsize=(20, 10))\nfor i in range(5):\n    ax[i].plot(x[i, 0, :])\n    ax[i].set_title(f'Label: {class_names[y[i].item()]}')\n    ax[i].set_xlabel('Time (ms)')\n    ax[i].set_ylabel('Amplitude (mV)')","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:24:04.070922Z","iopub.execute_input":"2023-01-09T09:24:04.071298Z","iopub.status.idle":"2023-01-09T09:24:04.69951Z","shell.execute_reply.started":"2023-01-09T09:24:04.071267Z","shell.execute_reply":"2023-01-09T09:24:04.698527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Network Architecture\n### Here, we define a Network-in-network arch to process signals\n### instead of using linear layers at the end of the model to classify the signals, we use 1x1 Conv1d layers to capture correlations and global average pooling and reduce the dims\n### Also, while experimenting I found out that GELU activation works better than RELU in this case","metadata":{}},{"cell_type":"code","source":"#make some convolutional blocks to use in the model\ndef nin_block(in_channels, out_channels, kernel_size, padding, strides):\n    return nn.Sequential(\n        nn.Conv1d(in_channels, out_channels, kernel_size, strides, padding),\n        nn.BatchNorm1d(out_channels),\n        nn.GELU(),\n        nn.Conv1d(out_channels, out_channels, kernel_size=1), nn.GELU(),\n        nn.Conv1d(out_channels, out_channels, kernel_size=1), nn.GELU()\n    )","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:24:13.99995Z","iopub.execute_input":"2023-01-09T09:24:14.000321Z","iopub.status.idle":"2023-01-09T09:24:14.009902Z","shell.execute_reply.started":"2023-01-09T09:24:14.000291Z","shell.execute_reply":"2023-01-09T09:24:14.008892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model():\n    return nn.Sequential( #input shape: (batch_size, 1, 187)\n        nin_block(1, 48, kernel_size=11, strides=4, padding=0), #output shape: (batch_size, 48, 44)\n        nn.MaxPool1d(3, stride=2), #output shape: (batch_size, 48, 21)\n        nin_block(48, 128, kernel_size=5, strides=1, padding=2), #output shape: (batch_size, 128, 21)\n        nn.MaxPool1d(3, stride=2), #output shape: (batch_size, 128, 10)\n        nin_block(128, 256, kernel_size=3, strides=1, padding=1), #output shape: (batch_size, 256, 10)\n        nn.MaxPool1d(3, stride=2), #output shape: (batch_size, 256, 4)\n        nn.Dropout(0.4), \n        #last layers for classification of 5 classes\n        nin_block(256, 5, kernel_size=3, strides=1, padding=1), #output shape: (batch_size, 5, 4)\n        nn.AdaptiveAvgPool1d(1), #output shape: (batch_size, 5, 1)\n        nn.Flatten() #output shape: (batch_size, 5)\n    )","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:24:36.666332Z","iopub.execute_input":"2023-01-09T09:24:36.667059Z","iopub.status.idle":"2023-01-09T09:24:36.674129Z","shell.execute_reply.started":"2023-01-09T09:24:36.667024Z","shell.execute_reply":"2023-01-09T09:24:36.672843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model()","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:24:39.594359Z","iopub.execute_input":"2023-01-09T09:24:39.594747Z","iopub.status.idle":"2023-01-09T09:24:39.610107Z","shell.execute_reply.started":"2023-01-09T09:24:39.594715Z","shell.execute_reply":"2023-01-09T09:24:39.608714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model size in MB\nprint(f'Model size: {sum(p.numel() for p in model.parameters() if p.requires_grad)*4/(1024**2)} MB')\n#number of trainable parameters\nprint(f'Number of trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}')","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:24:50.009088Z","iopub.execute_input":"2023-01-09T09:24:50.009448Z","iopub.status.idle":"2023-01-09T09:24:50.015912Z","shell.execute_reply.started":"2023-01-09T09:24:50.009414Z","shell.execute_reply":"2023-01-09T09:24:50.014633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#find device to run the model on\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:24:53.200381Z","iopub.execute_input":"2023-01-09T09:24:53.201174Z","iopub.status.idle":"2023-01-09T09:24:53.270788Z","shell.execute_reply.started":"2023-01-09T09:24:53.201137Z","shell.execute_reply":"2023-01-09T09:24:53.269629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#move the model to the device\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:24:57.137418Z","iopub.execute_input":"2023-01-09T09:24:57.138471Z","iopub.status.idle":"2023-01-09T09:25:00.287235Z","shell.execute_reply.started":"2023-01-09T09:24:57.138428Z","shell.execute_reply":"2023-01-09T09:25:00.286135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While experimenting, I found out that if I set some weights to some classes the model performs better.\n\nThis way I force the model to learn the classes that it didn't do well on.","metadata":{}},{"cell_type":"code","source":"#loss function\ncriterion = nn.CrossEntropyLoss(weight=torch.tensor([1, 1.1, 2.5, 3, 1], dtype=torch.float, device=device)) #CE loss automatically uses indices as targets","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:25:07.092056Z","iopub.execute_input":"2023-01-09T09:25:07.092412Z","iopub.status.idle":"2023-01-09T09:25:07.098165Z","shell.execute_reply.started":"2023-01-09T09:25:07.092382Z","shell.execute_reply":"2023-01-09T09:25:07.097085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=hyper_params['lr'])","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:25:14.988729Z","iopub.execute_input":"2023-01-09T09:25:14.989609Z","iopub.status.idle":"2023-01-09T09:25:14.995665Z","shell.execute_reply.started":"2023-01-09T09:25:14.989545Z","shell.execute_reply":"2023-01-09T09:25:14.99455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lr_scheduler\nlr_scheduler = StepLR(optimizer, step_size=10, gamma=hyper_params['lr_decay'], verbose=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:25:17.685654Z","iopub.execute_input":"2023-01-09T09:25:17.686954Z","iopub.status.idle":"2023-01-09T09:25:17.692955Z","shell.execute_reply.started":"2023-01-09T09:25:17.686907Z","shell.execute_reply":"2023-01-09T09:25:17.691738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training function","metadata":{}},{"cell_type":"code","source":"def train(model, criterion, optimizer, train_dl, val_dl, epochs, lr_scheduler: StepLR=None):\n    \"\"\"trains the model for the given number of epochs\n\n    Args:\n        model (nn.Module): nn model\n        criterion (nn.CELoss): loss function\n        optimizer (torch.optim): optimizer\n        train_dl (Dataloader): training dataloader\n        val_dl (Dataloader): validation dataloader\n        epochs (int): number of epochs to train for\n        lr_scheduler (StepLR): learning rate scheduler to adjust lr during training\n    Returns:\n        list of training losses and validation losses, best model parameters, best accuracy\n    \"\"\"\n    train_losses = []\n    val_losses = []\n    best_model_params = deepcopy(model.state_dict())\n    best_accuracy = 0.0\n    for epoch in range(epochs):\n        print(f'Epoch {epoch+1}/{epochs}')\n        print('-'*10)\n        train_loss = 0\n        val_loss = 0\n        model.train() #set the model to training mode\n        for x, y in train_dl: #x: (batch_size, 1, 187), y: (batch_size, 1)\n            x = x.to(device)\n            y = y.to(device)\n            optimizer.zero_grad()\n            output = model(x) #the output shape is (batch_size, 5) so it's a distribution over the 5 classes\n            loss = criterion(output, y.squeeze())\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()*x.size(0)\n        train_loss = train_loss/len(train_dl.dataset)\n        train_losses.append(train_loss)\n        lr_scheduler.step() #update the learning rate every n epochs\n        model.eval() #set the model to evaluation mode\n        corrects = 0 #in order to calculate accuracy we store the number of correct predictions\n        for x, y in val_dl:\n            x = x.to(device)\n            y = y.to(device)\n            output = model(x) #out shape: (batch_size, 5)\n            loss = criterion(output, y.squeeze())\n            val_loss += loss.item()*x.size(0)\n            #calculate the number of correct predictions\n            corrects += torch.sum(torch.argmax(output, dim=1) == y.squeeze()).item()\n        val_loss = val_loss/len(val_dl.dataset)\n        accuracy = corrects/len(val_dl.dataset)\n        val_losses.append(val_loss)\n        print(f'Train Loss: {train_loss:.4f} \\t Val Loss: {val_loss:.4f} \\t Val Accuracy: {accuracy:.4f}')\n        #if the model performs better on the validation set, save the model parameters\n        if accuracy > best_accuracy:\n            best_accuracy = accuracy\n            best_model_params = deepcopy(model.state_dict())\n    print('Finished Training and the best accuracy is: {:.4f}'.format(best_accuracy))\n    return train_losses, val_losses, best_model_params, best_accuracy","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:25:20.825557Z","iopub.execute_input":"2023-01-09T09:25:20.826086Z","iopub.status.idle":"2023-01-09T09:25:20.840316Z","shell.execute_reply.started":"2023-01-09T09:25:20.826053Z","shell.execute_reply":"2023-01-09T09:25:20.83917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_losses, val_losses, best_model_params, best_accuracy = train(model, criterion, optimizer, train_dl, val_dl, hyper_params['epochs'], lr_scheduler)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:25:26.741517Z","iopub.execute_input":"2023-01-09T09:25:26.744168Z","iopub.status.idle":"2023-01-09T09:27:41.332106Z","shell.execute_reply.started":"2023-01-09T09:25:26.744132Z","shell.execute_reply":"2023-01-09T09:27:41.330997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The model finishes training with a validation accuracy of 98.6% !","metadata":{}},{"cell_type":"code","source":"#plot the training and validation losses with smooth lines\nplt.plot(np.convolve(train_losses, np.ones(3)/3, mode='valid'), label='train')\nplt.plot(np.convolve(val_losses, np.ones(3)/3, mode='valid'), label='val')\nplt.legend()\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:27:52.97876Z","iopub.execute_input":"2023-01-09T09:27:52.979269Z","iopub.status.idle":"2023-01-09T09:27:53.202495Z","shell.execute_reply.started":"2023-01-09T09:27:52.979231Z","shell.execute_reply":"2023-01-09T09:27:53.201509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load the best model parameters\nmodel.load_state_dict(best_model_params)\n#save the model to a file\ntorch.save(model.state_dict(), f'best_model{best_accuracy:.2f}.pt')","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:28:05.261123Z","iopub.execute_input":"2023-01-09T09:28:05.261519Z","iopub.status.idle":"2023-01-09T09:28:05.279453Z","shell.execute_reply.started":"2023-01-09T09:28:05.261488Z","shell.execute_reply":"2023-01-09T09:28:05.278587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#use this cell to load weights from disk\nmodel = get_model().to(device=device)\nmodel.load_state_dict(torch.load('best_model0.99.pt'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#confusion matrix function\ndef plot_cm(model, dl, categories, normalize='true'):\n    #plot the confusion matrix\n    model.eval()\n    y_pred = []\n    y_true = []\n    for x, y in dl:\n        x = x.to(device)\n        y = y.to(device)\n        output = model(x) #out shape: (batch_size, 5)\n        y_pred.extend(torch.argmax(output, dim=1).cpu().numpy())\n        y_true.extend(y.cpu().numpy())\n    cm = confusion_matrix(y_true, y_pred, normalize=normalize)\n    sns.heatmap(cm, annot=True, fmt= '.2f', cmap='Blues', xticklabels=categories.values(), yticklabels=categories.values())\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.show()\n    print(classification_report(y_true, y_pred, target_names=class_names.values()))","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:28:24.126981Z","iopub.execute_input":"2023-01-09T09:28:24.12737Z","iopub.status.idle":"2023-01-09T09:28:24.137631Z","shell.execute_reply.started":"2023-01-09T09:28:24.127339Z","shell.execute_reply":"2023-01-09T09:28:24.136616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's not get too excited, we need to examine the results and make sure the model doesn't trick us by skipping minor classes","metadata":{}},{"cell_type":"code","source":"#plot the confusion matrix using the validation set\nplot_cm(model, val_dl, class_names)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:28:27.785593Z","iopub.execute_input":"2023-01-09T09:28:27.786547Z","iopub.status.idle":"2023-01-09T09:28:28.394566Z","shell.execute_reply.started":"2023-01-09T09:28:27.78651Z","shell.execute_reply":"2023-01-09T09:28:28.393492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We can see the model does excellently on the validation set","metadata":{}},{"cell_type":"code","source":"#visualize the predictions the model got wrong\nmodel.eval()\ncount = 0\nfor x, y in val_dl:\n    x = x.to(device)\n    y = y.to(device)\n    output = model(x) #out shape: (batch_size, 5)\n    y_pred = torch.argmax(output, dim=1)\n    for i in range(len(y_pred)):\n        if y_pred[i] != y[i] and count < 5:\n            print(f'True: {class_names[y[i].item()]} \\t Predicted: {class_names[y_pred[i].item()]}')\n            plt.plot(x[i, 0, :].cpu().numpy())\n            plt.show()\n            count += 1","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:29:14.980753Z","iopub.execute_input":"2023-01-09T09:29:14.981165Z","iopub.status.idle":"2023-01-09T09:29:16.408683Z","shell.execute_reply.started":"2023-01-09T09:29:14.981116Z","shell.execute_reply":"2023-01-09T09:29:16.40762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---------------------------------------------------------------------------------------------------------------------------------------------\n# this section calculates the model accuracy on **test** data","metadata":{}},{"cell_type":"code","source":"#this section loads the test data and calculates the accuracy on the test set\ntest_df = pd.read_csv('/kaggle/input/heartbeat/mitbih_test.csv', header=None)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:29:27.192293Z","iopub.execute_input":"2023-01-09T09:29:27.19268Z","iopub.status.idle":"2023-01-09T09:29:29.132896Z","shell.execute_reply.started":"2023-01-09T09:29:27.192647Z","shell.execute_reply":"2023-01-09T09:29:29.131907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the last column is the label\nlabel_df = test_df.iloc[:, -1]\nlabel_df = label_df.astype('int')\nlabel_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:29:32.655232Z","iopub.execute_input":"2023-01-09T09:29:32.655607Z","iopub.status.idle":"2023-01-09T09:29:32.666291Z","shell.execute_reply.started":"2023-01-09T09:29:32.655574Z","shell.execute_reply":"2023-01-09T09:29:32.664763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the rest of the columns are the signal\nfeatures_df = test_df.iloc[:, :-1]\nfeatures_df = features_df.astype('float32')\nfeatures_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:29:35.596423Z","iopub.execute_input":"2023-01-09T09:29:35.597148Z","iopub.status.idle":"2023-01-09T09:29:35.635325Z","shell.execute_reply.started":"2023-01-09T09:29:35.597108Z","shell.execute_reply":"2023-01-09T09:29:35.634221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#convert to numpy arrays\nX_test_np = features_df.to_numpy()\ny_test_np = label_df.to_numpy()","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:29:39.311584Z","iopub.execute_input":"2023-01-09T09:29:39.312133Z","iopub.status.idle":"2023-01-09T09:29:39.317541Z","shell.execute_reply.started":"2023-01-09T09:29:39.312093Z","shell.execute_reply":"2023-01-09T09:29:39.31662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_np.shape, y_test_np.shape","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:29:42.213065Z","iopub.execute_input":"2023-01-09T09:29:42.213421Z","iopub.status.idle":"2023-01-09T09:29:42.220128Z","shell.execute_reply.started":"2023-01-09T09:29:42.213387Z","shell.execute_reply":"2023-01-09T09:29:42.219102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we have to reshape the data to be (batch_size, 1, 187) because that's the shape the model expects\nX_test_np = np.reshape(X_test_np, (X_test_np.shape[0], 1, X_test_np.shape[1]))\nX_test_np.shape","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:29:45.354298Z","iopub.execute_input":"2023-01-09T09:29:45.354692Z","iopub.status.idle":"2023-01-09T09:29:45.361977Z","shell.execute_reply.started":"2023-01-09T09:29:45.354659Z","shell.execute_reply":"2023-01-09T09:29:45.360829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_np = np.reshape(y_test_np, (y_test_np.shape[0], 1))\ny_test_np.shape","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:29:47.98616Z","iopub.execute_input":"2023-01-09T09:29:47.986803Z","iopub.status.idle":"2023-01-09T09:29:47.994501Z","shell.execute_reply.started":"2023-01-09T09:29:47.986764Z","shell.execute_reply":"2023-01-09T09:29:47.993332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we have to normalize the data\nX_test_np = (X_test_np - X_test_np.mean())/X_test_np.std()\nX_test_np[:5, 0, :5]","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:29:50.969195Z","iopub.execute_input":"2023-01-09T09:29:50.969551Z","iopub.status.idle":"2023-01-09T09:29:50.997985Z","shell.execute_reply.started":"2023-01-09T09:29:50.969521Z","shell.execute_reply":"2023-01-09T09:29:50.996807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visualize the first 5 samples\nfor i in range(5):\n    plt.plot(X_test_np[i, 0])\n    plt.title(class_names[y_test_np[i, 0]])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:29:56.125048Z","iopub.execute_input":"2023-01-09T09:29:56.125416Z","iopub.status.idle":"2023-01-09T09:29:56.998891Z","shell.execute_reply.started":"2023-01-09T09:29:56.125385Z","shell.execute_reply":"2023-01-09T09:29:56.997948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'test mean: {X_test_np.mean():.4f} \\t test std: {X_test_np.std():.4f}')","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:30:03.209198Z","iopub.execute_input":"2023-01-09T09:30:03.210223Z","iopub.status.idle":"2023-01-09T09:30:03.227427Z","shell.execute_reply.started":"2023-01-09T09:30:03.210182Z","shell.execute_reply":"2023-01-09T09:30:03.226241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print the number of samples in each class\nfor i in range(5):\n    print(f'class {i}: {np.sum(y_test_np == i)}')","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:30:07.274953Z","iopub.execute_input":"2023-01-09T09:30:07.275911Z","iopub.status.idle":"2023-01-09T09:30:07.283096Z","shell.execute_reply.started":"2023-01-09T09:30:07.275874Z","shell.execute_reply":"2023-01-09T09:30:07.281861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create a dataset\ntest_ds = TensorDataset(torch.from_numpy(X_test_np), torch.from_numpy(y_test_np))\ntest_dl = DataLoader(test_ds, batch_size=hyper_params['bs'], shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:30:12.724227Z","iopub.execute_input":"2023-01-09T09:30:12.724619Z","iopub.status.idle":"2023-01-09T09:30:12.730792Z","shell.execute_reply.started":"2023-01-09T09:30:12.724581Z","shell.execute_reply":"2023-01-09T09:30:12.729655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#use this cell to load weights from disk\nmodel = get_model().to(device=device)\nmodel.load_state_dict(torch.load('best_model0.99.pt'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#calculate the sensitivity and specificity for each class using scikit-learn\nplot_cm(model, test_dl, class_names)","metadata":{"execution":{"iopub.status.busy":"2023-01-09T09:30:20.933977Z","iopub.execute_input":"2023-01-09T09:30:20.93438Z","iopub.status.idle":"2023-01-09T09:30:21.484684Z","shell.execute_reply.started":"2023-01-09T09:30:20.934347Z","shell.execute_reply":"2023-01-09T09:30:21.483536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We see here that the model does as well as it did on the validation set, this indicates that the model has learned to generalize!","metadata":{}},{"cell_type":"markdown","source":"### We have a total accuracy of 98% on the test data\n#### also we have to pay attention to precision and recall\n#### we don't have the numbers we had with the validation set but still it does very well","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}